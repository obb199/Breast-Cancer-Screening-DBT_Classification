{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import cv2\n","from tensorflow import keras\n","import os\n","from sklearn import model_selection\n","from sklearn import utils\n","from sklearn.metrics import confusion_matrix, roc_curve, RocCurveDisplay, roc_auc_score\n","from scipy import ndimage\n","from gc import collect\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from seaborn import heatmap\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:34.196944Z","iopub.execute_input":"2023-05-30T18:03:34.197926Z","iopub.status.idle":"2023-05-30T18:03:43.014163Z","shell.execute_reply.started":"2023-05-30T18:03:34.197883Z","shell.execute_reply":"2023-05-30T18:03:43.013077Z"},"trusted":true,"id":"zwLE8GWFhr2h","outputId":"7785b429-5447-4c9f-8614-973ee0458bfb"},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":["def get_labels(label_file):\n","    \"\"\"lê a tabela com as informações dos pacientes e retorna uma matriz com o ID e as labels\"\"\"\n","    labels = pd.read_csv(label_file)\n","    cancer_labels = dict()\n","\n","    for p in labels.index:\n","        cancer_labels[labels['PatientID'][p]] = [int(labels['Normal'][p]), int(labels['Actionable'][p]), int(labels['Benign'][p]), int(labels['Cancer'][p])]\n","\n","    return cancer_labels"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.020123Z","iopub.execute_input":"2023-05-30T18:03:43.022745Z","iopub.status.idle":"2023-05-30T18:03:43.031306Z","shell.execute_reply.started":"2023-05-30T18:03:43.022708Z","shell.execute_reply":"2023-05-30T18:03:43.030382Z"},"trusted":true,"id":"E6kjxoDQhr2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = get_labels('/kaggle/input/labelsssss/labels.csv')\n","numbers_per_class = [0, 0, 0, 0]\n","for i in labels:\n","    numbers_per_class[np.argmax(labels[i])] += 1\n","\n","proportion_per_class = [round(number_of_class/sum(numbers_per_class), 2) for number_of_class in numbers_per_class]\n","proportion_per_class"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.035647Z","iopub.execute_input":"2023-05-30T18:03:43.038093Z","iopub.status.idle":"2023-05-30T18:03:43.131195Z","shell.execute_reply.started":"2023-05-30T18:03:43.038059Z","shell.execute_reply":"2023-05-30T18:03:43.130283Z"},"trusted":true,"id":"bEaULKwjhr2m","outputId":"ea430113-2c50-415c-bf91-342eeb42bef7"},"execution_count":null,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[0.35, 0.25, 0.23, 0.17]"},"metadata":{}}]},{"cell_type":"code","source":["path = '/kaggle/input/192x256xdepth/'\n","dirs = os.listdir(path)\n","\n","X = [path + i for i in os.listdir(path)]\n","y = [np.argmax(labels[(path+i)[-14:-4]]) for i in os.listdir(path)]"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.137049Z","iopub.execute_input":"2023-05-30T18:03:43.139573Z","iopub.status.idle":"2023-05-30T18:03:43.215728Z","shell.execute_reply.started":"2023-05-30T18:03:43.139535Z","shell.execute_reply":"2023-05-30T18:03:43.214805Z"},"trusted":true,"id":"1H-FtkuWhr2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def balancing_batch(X, y, max_value):\n","    numbers_per_class = sorted(Counter(y).items())\n","\n","    if len(numbers_per_class) == 1:\n","        return X[0:1], y[0:1] #return only the first image because the batch has only one class\n","\n","    X, y = utils.shuffle(X, y)\n","    new_X, new_y = [], []\n","    counter_class_zero = 0\n","    counter_class_one = 0\n","    counter_class_two = 0\n","    counter_class_three = 0\n","\n","    max_per_class = int(max_value/4)\n","\n","    for test_x, test_y in zip(X, y):\n","        if test_y == 0 and counter_class_zero < max_per_class:\n","            new_X.append(test_x)\n","            new_y.append(test_y)\n","            counter_class_zero += 1\n","        elif test_y == 1 and counter_class_one < max_per_class:\n","            new_X.append(test_x)\n","            new_y.append(test_y)\n","            counter_class_one += 1\n","        elif test_y == 2 and counter_class_two < max_per_class:\n","            new_X.append(test_x)\n","            new_y.append(test_y)\n","            counter_class_two += 1\n","        elif test_y == 3 and counter_class_three < max_per_class:\n","            new_X.append(test_x)\n","            new_y.append(test_y)\n","            counter_class_three += 1\n","\n","    return np.array(new_X, dtype='float16'), np.array(new_y, dtype='uint8')"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.217225Z","iopub.execute_input":"2023-05-30T18:03:43.217575Z","iopub.status.idle":"2023-05-30T18:03:43.231855Z","shell.execute_reply.started":"2023-05-30T18:03:43.217542Z","shell.execute_reply":"2023-05-30T18:03:43.230765Z"},"trusted":true,"id":"bpTcgO6Yhr2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def separate_slices(img):\n","    'function to separate 2d images of 3d original image'\n","    slices = []\n","\n","    for i in range(img.shape[-2]):\n","        slices.append(np.array(img[:, :, i]))\n","\n","    slices.append(np.mean(img, axis=-2)) #including mean of slices\n","\n","    return slices"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.233747Z","iopub.execute_input":"2023-05-30T18:03:43.234297Z","iopub.status.idle":"2023-05-30T18:03:43.247442Z","shell.execute_reply.started":"2023-05-30T18:03:43.234260Z","shell.execute_reply":"2023-05-30T18:03:43.246604Z"},"trusted":true,"id":"oYey-3wthr2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y,random_state=42, train_size=0.8)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.248853Z","iopub.execute_input":"2023-05-30T18:03:43.249558Z","iopub.status.idle":"2023-05-30T18:03:43.262336Z","shell.execute_reply.started":"2023-05-30T18:03:43.249527Z","shell.execute_reply":"2023-05-30T18:03:43.261211Z"},"trusted":true,"id":"hu2wG_2Ahr2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataGenerator(tf.keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, labels_dir, batch_size, sub_batch_size, dim_img, training, shuffle=True):\n","        self.list_IDs = list_IDs # array of strings with original images name with directory\n","        self.labels = self.__get_labels(labels_dir) #dict with labels of all images\n","        self.batch_size = batch_size #3d-images per batch\n","        self.sub_batch_size = sub_batch_size #quantity of sub-images per batch will be choose to train\n","        self.dim_img = dim_img # tuple with width and height of image like (192, 256)\n","        self.training = training # true if generator is for training, false if generator is for validation\n","        self.shuffle = shuffle # true or false to shuffle data after any epochs\n","        self.on_epoch_end() # call of the function\n","\n","\n","    def __get_labels(self, label_file):\n","        'take the dict with labels of images'\n","        labels = pd.read_csv(label_file)\n","        cancer_labels = dict()\n","\n","        for p in labels.index:\n","            cancer_labels[labels['PatientID'][p]] = [int(labels['Normal'][p]), int(labels['Actionable'][p]), int(labels['Benign'][p]), int(labels['Cancer'][p])]\n","\n","        return cancer_labels\n","\n","    def __data_augmentation(self, x):\n","        'generate variations of images'\n","        new_images = []\n","        x = x.astype('float16')\n","        new_images.append(x)\n","\n","        x = cv2.flip(x.astype('float32'), 1).astype('float16')\n","\n","        new_images.append(np.expand_dims(x, -1))\n","\n","        return utils.shuffle(new_images)\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(list_IDs_temp)\n","\n","        if self.training:\n","            X, y = balancing_batch(X, y, self.sub_batch_size)\n","            return np.array(X[0:self.sub_batch_size], dtype='float16'), np.array(y[0:self.sub_batch_size], dtype='uint8')\n","\n","        return np.array(X), np.array(y)\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples'\n","        X = []\n","        y = []\n","\n","        # Generate data\n","        for i, ID in enumerate(list_IDs_temp):\n","            # Store sample\n","            prev_len_X = len(X)\n","            full_image = np.load(ID)\n","            new_images = separate_slices(full_image)\n","            if self.training:\n","                for img in new_images:\n","                    X += self.__data_augmentation(img)\n","            else:\n","                X = np.array(new_images, dtype='float16')\n","\n","            #adding new data labels for y array\n","            for _ in range(len(X) - prev_len_X):\n","                y.append(self.labels[ID[-14:-4]]) #'-14:-4 represent a part of string with name of original image that slices was taken'\n","\n","        X, y = utils.shuffle(X, y)\n","        return X, y"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.264083Z","iopub.execute_input":"2023-05-30T18:03:43.264467Z","iopub.status.idle":"2023-05-30T18:03:43.286609Z","shell.execute_reply.started":"2023-05-30T18:03:43.264432Z","shell.execute_reply":"2023-05-30T18:03:43.285760Z"},"trusted":true,"id":"4213LKk-hr2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InceptionModule(keras.layers.Layer):\n","    def __init__(self, filters, activation='relu', **kwargs):\n","        super().__init__(**kwargs)\n","        self.activation = keras.activations.get('relu')\n","\n","        self.conv_a    = keras.layers.Conv2D(filters[0], kernel_size=(1,1), padding='same', use_bias='false', strides=1)\n","        self.bn_a      = keras.layers.BatchNormalization()\n","\n","        self.conv_b1   = keras.layers.Conv2D(filters[1], kernel_size=(1, 1), padding='same', use_bias='false', strides=1)\n","        self.conv_b2   = keras.layers.Conv2D(filters[2], kernel_size=(3, 4), padding='same', use_bias='false', strides=1)\n","        self.bn_b      = keras.layers.BatchNormalization()\n","\n","        self.conv_c1   = keras.layers.Conv2D(filters[3], kernel_size=(1, 1), padding='same', use_bias='false', strides=1)\n","        self.conv_c2   = keras.layers.Conv2D(filters[4], kernel_size=(6, 8), padding='same', use_bias='false', strides=1)\n","        self.bn_c      = keras.layers.BatchNormalization()\n","\n","        self.maxpool_d = keras.layers.MaxPooling2D(pool_size=(3, 4), padding='same', strides=1)\n","        self.conv_d    = keras.layers.Conv2D(filters[5], kernel_size=(1, 1), padding='same', use_bias='false', strides=1)\n","        self.bn_d      = keras.layers.BatchNormalization()\n","\n","    def call(self, x):\n","        out1 = self.conv_a(x)\n","        out1 = self.bn_a(out1)\n","        out1 = self.activation(out1)\n","\n","        out2 = self.conv_b1(x)\n","        out2 = self.conv_b2(out2)\n","        out2 = self.bn_b(out2)\n","        out2 = self.activation(out2)\n","\n","        out3 = self.conv_c1(x)\n","        out3 = self.conv_c2(out3)\n","        out3 = self.bn_c(out3)\n","        out3 = self.activation(out3)\n","\n","        out4 = self.maxpool_d(x)\n","        out4 = self.conv_d(out4)\n","        out4 = self.bn_d(out4)\n","        out4 = self.activation(out4)\n","\n","        return tf.concat([out1, out2, out3, out4], axis=3)\n","\n","def get_googlenet():\n","    GoogLeNet = keras.Sequential()\n","\n","    GoogLeNet.add(keras.layers.Conv2D(filters=64, kernel_size=(6, 8), strides=2, padding='same', input_shape=(192, 256, 1)))\n","    GoogLeNet.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n","    GoogLeNet.add(keras.layers.Lambda(lambda x: tf.nn.local_response_normalization(x)))\n","    GoogLeNet.add(keras.layers.Conv2D(filters=64, kernel_size=(1, 1), strides=1, padding='same'))\n","    GoogLeNet.add(keras.layers.Conv2D(filters=192, kernel_size=(3, 4), strides=1, padding='same'))\n","    GoogLeNet.add(keras.layers.Lambda(lambda x: tf.nn.local_response_normalization(x)))\n","    GoogLeNet.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n","    GoogLeNet.add(InceptionModule([64, 96, 128, 16, 32, 32]))\n","    GoogLeNet.add(InceptionModule([128, 128, 192, 32, 96, 64]))\n","    GoogLeNet.add(keras.layers.Lambda(lambda x: tf.nn.local_response_normalization(x)))\n","    GoogLeNet.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n","    GoogLeNet.add(InceptionModule([192, 96, 208, 16, 48, 64]))\n","    GoogLeNet.add(InceptionModule([160, 112, 224, 24, 64, 64]))\n","    GoogLeNet.add(InceptionModule([128, 128, 256, 24, 64, 64]))\n","    GoogLeNet.add(InceptionModule([112, 144, 288, 32, 64, 64]))\n","    GoogLeNet.add(InceptionModule([256, 160, 320, 32, 128, 128]))\n","    GoogLeNet.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n","    GoogLeNet.add(InceptionModule([64, 96, 128, 16, 32, 32]))\n","    GoogLeNet.add(InceptionModule([64, 96, 128, 16, 32, 32]))\n","    GoogLeNet.add(keras.layers.GlobalAveragePooling2D())\n","    GoogLeNet.add(keras.layers.Dropout(0.4))\n","    GoogLeNet.add(keras.layers.Dense(1024, activation='relu'))\n","    GoogLeNet.add(keras.layers.Dense(4, activation='sigmoid'))\n","\n","    return GoogLeNet"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:03:43.287900Z","iopub.execute_input":"2023-05-30T18:03:43.288267Z","iopub.status.idle":"2023-05-30T18:03:43.313456Z","shell.execute_reply.started":"2023-05-30T18:03:43.288235Z","shell.execute_reply":"2023-05-30T18:03:43.312507Z"},"trusted":true,"id":"BZnS-Pl6hr2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TRAINING**"],"metadata":{"id":"NWS0d-GEhr2r"}},{"cell_type":"code","source":["checkpoint_callback_auc1 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/normal_auc1/',\n","                                                             save_weights_only=True,\n","                                                             monitor='val_AUC_1',\n","                                                             mode='max',\n","                                                             save_best_only=True)\n","checkpoint_callback_auc2 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/normal_auc2/',\n","                                                             save_weights_only=True,\n","                                                             monitor='val_AUC_2',\n","                                                             mode='max',\n","                                                             save_best_only=True)\n","checkpoint_callback_auc3 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/normal_auc3/',\n","                                                             save_weights_only=True,\n","                                                             monitor='val_AUC_3',\n","                                                             mode='max',\n","                                                             save_best_only=True)\n","checkpoint_callback_auc4 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/normal_auc4/',\n","                                                             save_weights_only=True,\n","                                                             monitor='val_AUC_4',\n","                                                             mode='max',\n","                                                             save_best_only=True)\n","checkpoint_callback_auc5 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/normal_auc5/',\n","                                                             save_weights_only=True,\n","                                                             monitor='val_AUC_5',\n","                                                             mode='max',\n","                                                             save_best_only=True)\n","\n","callbacks_list = [checkpoint_callback_auc1,\n","                  checkpoint_callback_auc2,\n","                  checkpoint_callback_auc3,\n","                  checkpoint_callback_auc4,\n","                  checkpoint_callback_auc5]\n","\n","def lr_scheduler(epoch, lr):\n","    return lr*0.9\n","\n","lr_decay_function = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:48:29.967313Z","iopub.execute_input":"2023-05-22T15:48:29.967683Z","iopub.status.idle":"2023-05-22T15:48:29.976174Z","shell.execute_reply.started":"2023-05-22T15:48:29.967651Z","shell.execute_reply":"2023-05-22T15:48:29.975303Z"},"trusted":true,"id":"R0JCQUDrhr2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["histories = []\n","n_splits = 5\n","skf = model_selection.StratifiedShuffleSplit(n_splits=n_splits, random_state=314, train_size=0.85)\n","for number_of_split, data in enumerate(skf.split(X_train, y_train)):\n","    print(f'SPLIT {number_of_split+1}/{n_splits}:')\n","    train = [path + dirs[i] for i in data[0]]\n","    val = [path + dirs[j] for j in data[1]]\n","\n","    # Generators\n","    training_generator = DataGenerator(list_IDs=train,\n","                                       labels_dir='/kaggle/input/labelsssss/labels.csv',\n","                                       dim_img=(192, 256),\n","                                       batch_size=5,\n","                                       sub_batch_size=400,\n","                                       shuffle=True,\n","                                       training=True)\n","\n","    validation_generator = DataGenerator(list_IDs=val,\n","                                         labels_dir='/kaggle/input/labelsssss/labels.csv',\n","                                         dim_img=(192, 256),\n","                                         batch_size=1,\n","                                         sub_batch_size='IGNORED', #this argument will be ignored because training is false.\n","                                         shuffle=True,\n","                                         training=False)\n","\n","    googlenet = get_googlenet()\n","\n","    googlenet.compile(loss='categorical_crossentropy',\n","                     optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","                     metrics=[keras.metrics.AUC(name=f'AUC_{number_of_split+1}'),\n","                              keras.metrics.Accuracy(name='accuracy'),\n","                              keras.metrics.Precision(name='precision'),\n","                              keras.metrics.Recall(name='recall')])\n","\n","\n","    # Train model on dataset\n","    histories.append(googlenet.fit(training_generator,\n","                                   validation_data=validation_generator,\n","                                   epochs=35,\n","                                   use_multiprocessing=True,\n","                                   workers=1,\n","                                   callbacks=[callbacks_list[number_of_split], lr_decay_function]))\n","    print('\\n')\n","    collect()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-22T15:48:32.098695Z","iopub.execute_input":"2023-05-22T15:48:32.099044Z","iopub.status.idle":"2023-05-22T19:25:26.848749Z","shell.execute_reply.started":"2023-05-22T15:48:32.099015Z","shell.execute_reply":"2023-05-22T19:25:26.838491Z"},"trusted":true,"id":"rzShtz9chr2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(25, 50))\n","plt.subplot(4, 2, 1)\n","for i, h in enumerate(histories):\n","    plt.plot(list(range(1, 36)), h.history['loss'], marker='o')\n","    plt.title('GoogLeNet Loss evolution - Training')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('Loss on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n","\n","plt.subplot(4, 2, 2)\n","for i, h in enumerate(histories):\n","    plt.plot(list(range(1, 36)), h.history['val_loss'], marker='o')\n","    plt.title('GoogLeNet Loss evolution - Validations')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('Loss on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n","\n","plt.subplot(4, 2, 3)\n","for i, h in enumerate(histories):\n","    plt.plot(list(range(1, 36)), h.history['accuracy'], marker='o')\n","    plt.title('GoogLeNet Accuracy evolution - Training')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('Accuracy on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n","\n","plt.subplot(4, 2, 4)\n","for i, h in enumerate(histories):\n","    plt.plot(list(range(1, 36)), h.history['val_accuracy'], marker='o')\n","    plt.title('GoogLeNet Accuracy evolution - Validations')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('Accuracy on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n","\n","plt.subplot(4, 2, 5)\n","for i, h in enumerate(histories):\n","    plt.plot(list(range(1, 36)), h.history['recall'], marker='o')\n","    plt.title('GoogLeNet Recall evolution - Training')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('Recall on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n","\n","plt.subplot(4, 2, 6)\n","for i, h in enumerate(histories):\n","    plt.plot(list(range(1, 36)), h.history['val_recall'], marker='o')\n","    plt.title('GoogLeNet Recall evolution - Validations')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('Recall on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n","\n","plt.subplot(4, 2, 7)\n","for i, h in enumerate(histories):\n","    key_auc = f\"AUC_{i+1}\"\n","    plt.plot(list(range(1, 36)), h.history[key_auc], marker='o')\n","    plt.title('GoogLeNet AUC evolution - Training')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('AUC on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n","\n","plt.subplot(4, 2, 8)\n","for i, h in enumerate(histories):\n","    key_auc_val = f\"val_AUC_{i+1}\"\n","    plt.plot(list(range(1, 36)), h.history[key_auc_val], marker='o')\n","    plt.title('GoogLeNet AUC evolution - Validations')\n","    plt.xlabel('epoch number')\n","    plt.ylabel('AUC on epoch')\n","    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])"],"metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:25:26.854138Z","iopub.execute_input":"2023-05-22T19:25:26.855599Z","iopub.status.idle":"2023-05-22T19:25:31.182355Z","shell.execute_reply.started":"2023-05-22T19:25:26.855560Z","shell.execute_reply":"2023-05-22T19:25:31.181134Z"},"trusted":true,"id":"G3gcxkOehr2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Best results in validations for any k-fold: ')\n","for i, h in enumerate(histories):\n","    print(f'K-FOLD {i+1}:')\n","    print(\"TRAINING RESULTS:\")\n","    k = np.max(h.history[f'AUC_{i+1}'])\n","    print(f'Best AUC in train: {k}')\n","    k = np.max(h.history[f'accuracy'])\n","    print(f'Best Accuracy in train: {k}')\n","    k = np.max(h.history[f'precision'])\n","    print(f'Best Precision in train: {k}')\n","    k = np.max(h.history[f'recall'])\n","    print(f'Best Recall in train: {k}')\n","\n","    print(\"\\nVALIDATION RESULTS:\")\n","    k = np.max(h.history[f'val_AUC_{i+1}'])\n","    print(f'Best AUC in validation: {k}')\n","    k = np.max(h.history[f'val_accuracy'])\n","    print(f'Best Accuracy in validation: {k}')\n","    k = np.max(h.history[f'val_precision'])\n","    print(f'Best Precision in validation: {k}')\n","    k = np.max(h.history[f'val_recall'])\n","    print(f'Best Recall in validation: {k}')\n","    print()\n","    print(f'{50*\"=\"}')\n","    print()\n","\n","results = np.empty((4, 5))\n","for i, h in enumerate(histories):\n","    results[0][i] = np.max(h.history[f'val_AUC_{i+1}'])\n","    results[1][i] = np.max(h.history['val_accuracy'])\n","    results[2][i] = np.max(h.history['val_precision'])\n","    results[3][i] = np.max(h.history['val_recall'])\n","\n","print(f\"Average best AUC: {np.mean(results[0])}\")\n","print(f\"standard deviation AUC: {np.std(results[0])}\\n\")\n","print(f\"Average best Accuracy: {np.mean(results[1])}\")\n","print(f\"Standard Deviation Accuracy: {np.std(results[1])}\\n\")\n","print(f\"Average best Precision: {np.mean(results[2])}\")\n","print(f\"Standard Deviation Precision: {np.std(results[2])}\\n\")\n","print(f\"Average best Recall: {np.mean(results[3])}\")\n","print(f\"Standard Deviation Recall: {np.std(results[3])}\\n\")\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:25:31.183547Z","iopub.execute_input":"2023-05-22T19:25:31.183901Z","iopub.status.idle":"2023-05-22T19:25:31.201419Z","shell.execute_reply.started":"2023-05-22T19:25:31.183870Z","shell.execute_reply":"2023-05-22T19:25:31.200595Z"},"trusted":true,"id":"hAaI9PTHhr2u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TEST**"],"metadata":{"id":"CCE3ACBdhr2u"}},{"cell_type":"code","source":["training_generator = DataGenerator(list_IDs=X_train,\n","                                   labels_dir='/kaggle/input/labelsssss/labels.csv',\n","                                   dim_img=(192, 256),\n","                                   batch_size=5,\n","                                   sub_batch_size=400,\n","                                   shuffle=True,\n","                                   training=True)\n","\n","googlenet = get_googlenet()\n","\n","googlenet.compile(loss='binary_crossentropy',\n","                  optimizer=keras.optimizers.Adam(8e-7),\n","                  metrics=[keras.metrics.AUC(name='AUC'),\n","                           keras.metrics.BinaryAccuracy(name='accuracy'),\n","                           keras.metrics.Precision(name='precision'),\n","                           keras.metrics.Recall(name='recall')])\n","\n","\n","# Train model on dataset\n","googlenet.load_weights('/kaggle/working/models/cancer_auc4/') #start with best AUC in validations\n","history = googlenet.fit(training_generator, epochs=10)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:13:07.933111Z","iopub.execute_input":"2023-05-30T18:13:07.934114Z","iopub.status.idle":"2023-05-30T18:20:19.416594Z","shell.execute_reply.started":"2023-05-30T18:13:07.934071Z","shell.execute_reply":"2023-05-30T18:20:19.415532Z"},"trusted":true,"id":"sjiHACeshr28","outputId":"471b316a-f007-4bc5-859c-5282425055f7"},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n35/35 [==============================] - 74s 1s/step - loss: 0.5902 - AUC: 0.8622 - accuracy: 0.7710 - precision: 0.8116 - recall: 0.7027\nEpoch 2/10\n35/35 [==============================] - 37s 1s/step - loss: 0.3151 - AUC: 0.9521 - accuracy: 0.8829 - precision: 0.9175 - recall: 0.8393\nEpoch 3/10\n35/35 [==============================] - 38s 1s/step - loss: 0.4138 - AUC: 0.9018 - accuracy: 0.8293 - precision: 0.8393 - recall: 0.8114\nEpoch 4/10\n35/35 [==============================] - 43s 1s/step - loss: 0.2190 - AUC: 0.9714 - accuracy: 0.9257 - precision: 0.9071 - recall: 0.9476\nEpoch 5/10\n35/35 [==============================] - 41s 1s/step - loss: 0.2116 - AUC: 0.9734 - accuracy: 0.9440 - precision: 0.9375 - recall: 0.9506\nEpoch 6/10\n35/35 [==============================] - 38s 1s/step - loss: 0.2288 - AUC: 0.9746 - accuracy: 0.9076 - precision: 0.9137 - recall: 0.8988\nEpoch 7/10\n35/35 [==============================] - 38s 1s/step - loss: 0.2854 - AUC: 0.9598 - accuracy: 0.9000 - precision: 0.9279 - recall: 0.8660\nEpoch 8/10\n35/35 [==============================] - 38s 1s/step - loss: 0.1875 - AUC: 0.9815 - accuracy: 0.9203 - precision: 0.9304 - recall: 0.9075\nEpoch 9/10\n35/35 [==============================] - 38s 1s/step - loss: 0.1953 - AUC: 0.9737 - accuracy: 0.9277 - precision: 0.9057 - recall: 0.9538\nEpoch 10/10\n35/35 [==============================] - 38s 1s/step - loss: 0.1096 - AUC: 0.9949 - accuracy: 0.9756 - precision: 0.9748 - recall: 0.9762\n","output_type":"stream"}]},{"cell_type":"code","source":["res = []\n","for x in X_test:\n","    x = np.load(x)\n","    x = np.array(separate_slices(x))\n","    pred = googlenet.predict(np.array(x), verbose=0)\n","    res.append(np.argmax(pred))\n","print(res)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:20:19.418804Z","iopub.execute_input":"2023-05-30T18:20:19.419201Z","iopub.status.idle":"2023-05-30T18:21:05.364391Z","shell.execute_reply.started":"2023-05-30T18:20:19.419166Z","shell.execute_reply":"2023-05-30T18:21:05.363209Z"},"trusted":true,"id":"uFi0MiqEhr28","outputId":"b28ecaba-e630-4b51-aeff-aee2142f1ec0"},"execution_count":null,"outputs":[{"name":"stdout","text":"[0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":["matrix_confusion = np.array([[0, 0, 0, 0],\n","                             [0, 0, 0, 0],\n","                             [0, 0, 0, 0],\n","                             [0, 0, 0, 0]])\n","\n","for y_pred, y_true in zip(res, y_test):\n","    matrix_confusion[y_pred][y_true] += 1\n","\n","print(matrix_confusion)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-30T18:21:05.365904Z","iopub.execute_input":"2023-05-30T18:21:05.366700Z","iopub.status.idle":"2023-05-30T18:21:05.375938Z","shell.execute_reply.started":"2023-05-30T18:21:05.366658Z","shell.execute_reply":"2023-05-30T18:21:05.374688Z"},"trusted":true,"id":"VlkaQiF4hr29","outputId":"63e969f3-a3e5-4ebc-c419-241d827d4195"},"execution_count":null,"outputs":[{"name":"stdout","text":"[[16  2]\n [23  4]]\n0.4444444444444444\n","output_type":"stream"}]}]}