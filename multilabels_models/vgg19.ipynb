{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from sklearn import model_selection\n",
        "from sklearn import utils\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, RocCurveDisplay, roc_auc_score\n",
        "from scipy import ndimage\n",
        "from gc import collect\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from seaborn import heatmap\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:29.190953Z",
          "iopub.execute_input": "2023-06-29T15:54:29.191686Z",
          "iopub.status.idle": "2023-06-29T15:54:29.198554Z",
          "shell.execute_reply.started": "2023-06-29T15:54:29.191649Z",
          "shell.execute_reply": "2023-06-29T15:54:29.197239Z"
        },
        "trusted": true,
        "id": "cJYZ9oBIrbuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(label_file):\n",
        "    \"\"\"lê a tabela com as informações dos pacientes e retorna uma matriz com o ID e as labels\"\"\"\n",
        "    labels = pd.read_csv(label_file)\n",
        "    cancer_labels = dict()\n",
        "\n",
        "    for p in labels.index:\n",
        "        cancer_labels[labels['PatientID'][p]] = [int(labels['Normal'][p]), int(labels['Actionable'][p]), int(labels['Benign'][p]), int(labels['Cancer'][p])]\n",
        "\n",
        "    return cancer_labels"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:31.872335Z",
          "iopub.execute_input": "2023-06-29T15:54:31.872891Z",
          "iopub.status.idle": "2023-06-29T15:54:31.881680Z",
          "shell.execute_reply.started": "2023-06-29T15:54:31.872839Z",
          "shell.execute_reply": "2023-06-29T15:54:31.880739Z"
        },
        "trusted": true,
        "id": "8fZYTpbgrbuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = get_labels('/kaggle/input/labelsssss/labels.csv')\n",
        "numbers_per_class = [0, 0, 0, 0]\n",
        "for i in labels:\n",
        "    numbers_per_class[np.argmax(labels[i])] += 1\n",
        "\n",
        "proportion_per_class = [round(number_of_class/sum(numbers_per_class), 2) for number_of_class in numbers_per_class]\n",
        "proportion_per_class"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:33.854810Z",
          "iopub.execute_input": "2023-06-29T15:54:33.855267Z",
          "iopub.status.idle": "2023-06-29T15:54:34.033289Z",
          "shell.execute_reply.started": "2023-06-29T15:54:33.855232Z",
          "shell.execute_reply": "2023-06-29T15:54:34.032285Z"
        },
        "trusted": true,
        "id": "t10ZhSuzrbuF",
        "outputId": "aadc19b0-4949-469d-c7d8-e3287e38399a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[0.35, 0.25, 0.23, 0.17]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/kaggle/input/192x256xdepth/'\n",
        "dirs = os.listdir(path)\n",
        "\n",
        "X = [path + i for i in os.listdir(path)]\n",
        "y = [np.argmax(labels[(path+i)[-14:-4]]) for i in os.listdir(path)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:35.822073Z",
          "iopub.execute_input": "2023-06-29T15:54:35.822499Z",
          "iopub.status.idle": "2023-06-29T15:54:35.909204Z",
          "shell.execute_reply.started": "2023-06-29T15:54:35.822463Z",
          "shell.execute_reply": "2023-06-29T15:54:35.908280Z"
        },
        "trusted": true,
        "id": "4Z7R48uSrbuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balancing_batch(X, y, max_value):\n",
        "    numbers_per_class = sorted(Counter(y).items())\n",
        "\n",
        "    if len(numbers_per_class) == 1:\n",
        "        return X[0:1], y[0:1] #return only the first image because the batch has only one class\n",
        "\n",
        "    X, y = utils.shuffle(X, y)\n",
        "    new_X, new_y = [], []\n",
        "    counter_class_zero = 0\n",
        "    counter_class_one = 0\n",
        "    counter_class_two = 0\n",
        "    counter_class_three = 0\n",
        "\n",
        "    max_per_class = int(max_value/4)\n",
        "\n",
        "    for test_x, test_y in zip(X, y):\n",
        "        if test_y == 0 and counter_class_zero < max_per_class:\n",
        "            new_X.append(test_x)\n",
        "            new_y.append(test_y)\n",
        "            counter_class_zero += 1\n",
        "        elif test_y == 1 and counter_class_one < max_per_class:\n",
        "            new_X.append(test_x)\n",
        "            new_y.append(test_y)\n",
        "            counter_class_one += 1\n",
        "        elif test_y == 2 and counter_class_two < max_per_class:\n",
        "            new_X.append(test_x)\n",
        "            new_y.append(test_y)\n",
        "            counter_class_two += 1\n",
        "        elif test_y == 3 and counter_class_three < max_per_class:\n",
        "            new_X.append(test_x)\n",
        "            new_y.append(test_y)\n",
        "            counter_class_three += 1\n",
        "\n",
        "    return np.array(new_X, dtype='float16'), np.array(new_y, dtype='uint8')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:38.125859Z",
          "iopub.execute_input": "2023-06-29T15:54:38.126769Z",
          "iopub.status.idle": "2023-06-29T15:54:38.137862Z",
          "shell.execute_reply.started": "2023-06-29T15:54:38.126722Z",
          "shell.execute_reply": "2023-06-29T15:54:38.136738Z"
        },
        "trusted": true,
        "id": "mTgOKG_KrbuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_slices(img):\n",
        "    'function to separate 2d images of 3d original image'\n",
        "    slices = []\n",
        "\n",
        "    for i in range(img.shape[-2]):\n",
        "        slices.append(np.array(img[:, :, i]))\n",
        "\n",
        "    slices.append(np.mean(img, axis=-2)) #including mean of slices\n",
        "\n",
        "    return slices"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:40.334615Z",
          "iopub.execute_input": "2023-06-29T15:54:40.335016Z",
          "iopub.status.idle": "2023-06-29T15:54:40.341021Z",
          "shell.execute_reply.started": "2023-06-29T15:54:40.334984Z",
          "shell.execute_reply": "2023-06-29T15:54:40.339801Z"
        },
        "trusted": true,
        "id": "DWwaC-xmrbuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y,random_state=42, train_size=0.8)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:42.750888Z",
          "iopub.execute_input": "2023-06-29T15:54:42.751326Z",
          "iopub.status.idle": "2023-06-29T15:54:42.763475Z",
          "shell.execute_reply.started": "2023-06-29T15:54:42.751290Z",
          "shell.execute_reply": "2023-06-29T15:54:42.762268Z"
        },
        "trusted": true,
        "id": "vJeX1bKWrbuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels_dir, batch_size, sub_batch_size, dim_img, training, shuffle=True):\n",
        "        self.list_IDs = list_IDs # array of strings with original images name with directory\n",
        "        self.labels = self.__get_labels(labels_dir) #dict with labels of all images\n",
        "        self.batch_size = batch_size #3d-images per batch\n",
        "        self.sub_batch_size = sub_batch_size #quantity of sub-images per batch will be choose to train\n",
        "        self.dim_img = dim_img # tuple with width and height of image like (192, 256)\n",
        "        self.training = training # true if generator is for training, false if generator is for validation\n",
        "        self.shuffle = shuffle # true or false to shuffle data after any epochs\n",
        "        self.on_epoch_end() # call of the function\n",
        "\n",
        "    def __get_labels(self, label_file):\n",
        "        'take the dict with labels of images'\n",
        "        labels = pd.read_csv(label_file)\n",
        "        cancer_labels = dict()\n",
        "\n",
        "        for p in labels.index:\n",
        "            cancer_labels[labels['PatientID'][p]] = [int(labels['Normal'][p]), int(labels['Actionable'][p]), int(labels['Benign'][p]), int(labels['Cancer'][p])]\n",
        "\n",
        "        return cancer_labels\n",
        "\n",
        "    def __data_augmentation(self, x):\n",
        "        'generate variations of images'\n",
        "        new_images = []\n",
        "        x = x.astype('float16')\n",
        "        new_images.append(x)\n",
        "\n",
        "        x = cv2.flip(x.astype('float32'), 1).astype('float16')\n",
        "\n",
        "        new_images.append(np.expand_dims(x, -1))\n",
        "\n",
        "        return utils.shuffle(new_images)\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        if self.training:\n",
        "            X, y = balancing_batch(X, y, self.sub_batch_size)\n",
        "            return np.array(X[0:self.sub_batch_size], dtype='float16'), np.array(y[0:self.sub_batch_size], dtype='uint8')\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples'\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            prev_len_X = len(X)\n",
        "            full_image = np.load(ID)\n",
        "            new_images = separate_slices(full_image)\n",
        "            if self.training:\n",
        "                for img in new_images:\n",
        "                    X += self.__data_augmentation(img)\n",
        "            else:\n",
        "                X = np.array(new_images, dtype='float16')\n",
        "\n",
        "            #adding new data labels for y array\n",
        "            for _ in range(len(X) - prev_len_X):\n",
        "                y.append(self.labels[ID[-14:-4]]) #'-14:-4 represent a part of string with name of original image that slices was taken'\n",
        "\n",
        "        X, y = utils.shuffle(X, y)\n",
        "        return X, y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:44.720787Z",
          "iopub.execute_input": "2023-06-29T15:54:44.721170Z",
          "iopub.status.idle": "2023-06-29T15:54:44.739983Z",
          "shell.execute_reply.started": "2023-06-29T15:54:44.721142Z",
          "shell.execute_reply": "2023-06-29T15:54:44.738978Z"
        },
        "trusted": true,
        "id": "Eiq9LCACrbuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vgg19():\n",
        "    return tf.keras.Sequential([tf.keras.layers.Conv2D(filters=32, kernel_size=(6, 8),\n",
        "                                                       use_bias=True, padding='same', activation=\"relu\"),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "                                 tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "                                 tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 4), use_bias=True,\n",
        "                                                        padding='same', activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True, padding='same',\n",
        "                                                        activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True,\n",
        "                                                        padding='same', activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True,\n",
        "                                                        padding='same', activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True,\n",
        "                                                        padding='same', activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 4), use_bias=True,\n",
        "                                                        padding='same', activation=\"relu\",\n",
        "                                                        kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.MaxPool2D(pool_size=(3, 4)),\n",
        "                                 tf.keras.layers.Flatten(),\n",
        "                                 tf.keras.layers.Dense(1024, activation='relu'),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Dropout(0.4),\n",
        "                                 tf.keras.layers.Dense(1024, activation='relu'),\n",
        "                                 tf.keras.layers.BatchNormalization(),\n",
        "                                 tf.keras.layers.Dropout(0.4),\n",
        "                                 tf.keras.layers.Dense(4, activation='sigmoid')])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T15:54:48.858229Z",
          "iopub.execute_input": "2023-06-29T15:54:48.858598Z",
          "iopub.status.idle": "2023-06-29T15:54:48.881881Z",
          "shell.execute_reply.started": "2023-06-29T15:54:48.858568Z",
          "shell.execute_reply": "2023-06-29T15:54:48.880899Z"
        },
        "trusted": true,
        "id": "8xUfMAEerbuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback_auc1 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/cancer_auc1/',\n",
        "                                                             save_weights_only=True,\n",
        "                                                             monitor='val_AUC_1',\n",
        "                                                             mode='max',\n",
        "                                                             save_best_only=True)\n",
        "checkpoint_callback_auc2 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/cancer_auc2/',\n",
        "                                                             save_weights_only=True,\n",
        "                                                             monitor='val_AUC_2',\n",
        "                                                             mode='max',\n",
        "                                                             save_best_only=True)\n",
        "checkpoint_callback_auc3 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/cancer_auc3/',\n",
        "                                                             save_weights_only=True,\n",
        "                                                             monitor='val_AUC_3',\n",
        "                                                             mode='max',\n",
        "                                                             save_best_only=True)\n",
        "checkpoint_callback_auc4 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/cancer_auc4/',\n",
        "                                                             save_weights_only=True,\n",
        "                                                             monitor='val_AUC_4',\n",
        "                                                             mode='max',\n",
        "                                                             save_best_only=True)\n",
        "checkpoint_callback_auc5 = tf.keras.callbacks.ModelCheckpoint(filepath='./models/cancer_auc5/',\n",
        "                                                             save_weights_only=True,\n",
        "                                                             monitor='val_AUC_5',\n",
        "                                                             mode='max',\n",
        "                                                             save_best_only=True)\n",
        "\n",
        "callbacks_list = [checkpoint_callback_auc1,\n",
        "                  checkpoint_callback_auc2,\n",
        "                  checkpoint_callback_auc3,\n",
        "                  checkpoint_callback_auc4,\n",
        "                  checkpoint_callback_auc5]\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    return lr*0.9\n",
        "\n",
        "lr_decay_function = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T19:45:41.641871Z",
          "iopub.execute_input": "2023-06-29T19:45:41.642489Z",
          "iopub.status.idle": "2023-06-29T19:45:41.660629Z",
          "shell.execute_reply.started": "2023-06-29T19:45:41.642454Z",
          "shell.execute_reply": "2023-06-29T19:45:41.659507Z"
        },
        "trusted": true,
        "id": "iFlRzoZLrbuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histories = []\n",
        "n_splits = 5\n",
        "skf = model_selection.StratifiedShuffleSplit(n_splits=n_splits, random_state=314, train_size=0.85)\n",
        "for number_of_split, data in enumerate(skf.split(X_train, y_train)):\n",
        "    print(f'SPLIT {number_of_split+1}/{n_splits}:')\n",
        "    train = [path + dirs[i] for i in data[0]]\n",
        "    val = [path + dirs[j] for j in data[1]]\n",
        "\n",
        "    # Generators\n",
        "    training_generator = DataGenerator(list_IDs=train,\n",
        "                                       labels_dir='/kaggle/input/labelsssss/labels.csv',\n",
        "                                       dim_img=(192, 256),\n",
        "                                       batch_size=4,\n",
        "                                       sub_batch_size=300,\n",
        "                                       shuffle=True,\n",
        "                                       training=True)\n",
        "\n",
        "    validation_generator = DataGenerator(list_IDs=val,\n",
        "                                         labels_dir='/kaggle/input/labelsssss/labels.csv',\n",
        "                                         dim_img=(192, 256),\n",
        "                                         batch_size=1,\n",
        "                                         sub_batch_size='IGNORED', #this argument will be ignored because training is false.\n",
        "                                         shuffle=True,\n",
        "                                         training=False)\n",
        "\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    with strategy.scope():\n",
        "        vgg19 = get_vgg19()\n",
        "        vgg19.compile(loss='categorical_crossentropy',\n",
        "                         optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                         metrics=[keras.metrics.AUC(name=f'AUC_{number_of_split+1}'),\n",
        "                                  keras.metrics.Accuracy(name='accuracy'),\n",
        "                                  keras.metrics.Precision(name='precision'),\n",
        "                                  keras.metrics.Recall(name='recall')])\n",
        "\n",
        "\n",
        "    # Train model on dataset\n",
        "    histories.append(vgg19.fit(training_generator,\n",
        "                                   validation_data=validation_generator,\n",
        "                                   epochs=35,\n",
        "                                   use_multiprocessing=True,\n",
        "                                   workers=1,\n",
        "                                   callbacks=[callbacks_list[number_of_split], lr_decay_function]))\n",
        "    print('\\n')\n",
        "    collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-29T19:45:41.662754Z",
          "iopub.execute_input": "2023-06-29T19:45:41.663232Z",
          "iopub.status.idle": "2023-06-30T00:19:43.436283Z",
          "shell.execute_reply.started": "2023-06-29T19:45:41.663196Z",
          "shell.execute_reply": "2023-06-30T00:19:43.434395Z"
        },
        "trusted": true,
        "id": "C7ZHSDYUrbuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 50))\n",
        "plt.subplot(4, 2, 1)\n",
        "for i, h in enumerate(histories):\n",
        "    plt.plot(list(range(1, 36)), h.history['loss'], marker='o')\n",
        "    plt.title('VGG-19 Loss evolution - Training')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('Loss on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n",
        "\n",
        "plt.subplot(4, 2, 2)\n",
        "for i, h in enumerate(histories):\n",
        "    plt.plot(list(range(1, 36)), h.history['val_loss'], marker='o')\n",
        "    plt.title('VGG-19 Loss evolution - Validations')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('Loss on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n",
        "\n",
        "plt.subplot(4, 2, 3)\n",
        "for i, h in enumerate(histories):\n",
        "    plt.plot(list(range(1, 36)), h.history['accuracy'], marker='o')\n",
        "    plt.title('VGG-19 Accuracy evolution - Trainings')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('Accuracy on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n",
        "\n",
        "plt.subplot(4, 2, 4)\n",
        "for i, h in enumerate(histories):\n",
        "    plt.plot(list(range(1, 36)), h.history['val_accuracy'], marker='o')\n",
        "    plt.title('VGG-19 Accuracy evolution - Validations')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('Accuracy on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n",
        "\n",
        "plt.subplot(4, 2, 5)\n",
        "for i, h in enumerate(histories):\n",
        "    plt.plot(list(range(1, 36)), h.history['recall'], marker='o')\n",
        "    plt.title('VGG-19 Recall evolution - Training')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('Recall on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n",
        "\n",
        "plt.subplot(4, 2, 6)\n",
        "for i, h in enumerate(histories):\n",
        "    plt.plot(list(range(1, 36)), h.history['val_recall'], marker='o')\n",
        "    plt.title('VGG-19 Recall evolution - Validations')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('Recall on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n",
        "\n",
        "plt.subplot(4, 2, 7)\n",
        "for i, h in enumerate(histories):\n",
        "    key_auc = f\"AUC_{i+1}\"\n",
        "    plt.plot(list(range(1, 36)), h.history[key_auc], marker='o')\n",
        "    plt.title('VGG-19 AUC evolution - Training')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('AUC on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])\n",
        "\n",
        "plt.subplot(4, 2, 8)\n",
        "for i, h in enumerate(histories):\n",
        "    key_auc_val = f\"val_AUC_{i+1}\"\n",
        "    plt.plot(list(range(1, 36)), h.history[key_auc_val], marker='o')\n",
        "    plt.title('VGG-19 AUC evolution - Validations')\n",
        "    plt.xlabel('epoch number')\n",
        "    plt.ylabel('AUC on epoch')\n",
        "    plt.legend(['fold-1','fold-2', 'fold-3', 'fold-4', 'fold-5'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-30T00:19:43.443179Z",
          "iopub.execute_input": "2023-06-30T00:19:43.443505Z",
          "iopub.status.idle": "2023-06-30T00:19:46.993725Z",
          "shell.execute_reply.started": "2023-06-30T00:19:43.443477Z",
          "shell.execute_reply": "2023-06-30T00:19:46.992812Z"
        },
        "trusted": true,
        "id": "wKgSUcb2rbuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best results in validations for any k-fold: ')\n",
        "for i, h in enumerate(histories):\n",
        "    print(f'K-FOLD {i+1}:')\n",
        "    print(\"TRAINING RESULTS:\")\n",
        "    k = np.max(h.history[f'AUC_{i+1}'])\n",
        "    print(f'Best AUC in train: {k}')\n",
        "    k = np.max(h.history[f'accuracy'])\n",
        "    print(f'Best Accuracy in train: {k}')\n",
        "    k = np.max(h.history[f'precision'])\n",
        "    print(f'Best Precision in train: {k}')\n",
        "    k = np.max(h.history[f'recall'])\n",
        "    print(f'Best Recall in train: {k}')\n",
        "\n",
        "    print(\"\\nVALIDATION RESULTS:\")\n",
        "    k = np.max(h.history[f'val_AUC_{i+1}'])\n",
        "    print(f'Best AUC in validation: {k}')\n",
        "    k = np.max(h.history[f'val_accuracy'])\n",
        "    print(f'Best Accuracy in validation: {k}')\n",
        "    k = np.max(h.history[f'val_precision'])\n",
        "    print(f'Best Precision in validation: {k}')\n",
        "    k = np.max(h.history[f'val_recall'])\n",
        "    print(f'Best Recall in validation: {k}')\n",
        "    print()\n",
        "    print(f'{50*\"=\"}')\n",
        "    print()\n",
        "\n",
        "results = np.empty((4, 5))\n",
        "for i, h in enumerate(histories):\n",
        "    results[0][i] = np.max(h.history[f'val_AUC_{i+1}'])\n",
        "    results[1][i] = np.max(h.history['val_accuracy'])\n",
        "    results[2][i] = np.max(h.history['val_precision'])\n",
        "    results[3][i] = np.max(h.history['val_recall'])\n",
        "\n",
        "print(f\"Average best AUC: {np.mean(results[0])}\")\n",
        "print(f\"standard deviation AUC: {np.std(results[0])}\\n\")\n",
        "print(f\"Average best Accuracy: {np.mean(results[1])}\")\n",
        "print(f\"Standard Deviation Accuracy: {np.std(results[1])}\\n\")\n",
        "print(f\"Average best Precision: {np.mean(results[2])}\")\n",
        "print(f\"Standard Deviation Precision: {np.std(results[2])}\\n\")\n",
        "print(f\"Average best Recall: {np.mean(results[3])}\")\n",
        "print(f\"Standard Deviation Recall: {np.std(results[3])}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-30T00:19:46.995370Z",
          "iopub.execute_input": "2023-06-30T00:19:46.995998Z",
          "iopub.status.idle": "2023-06-30T00:19:47.015723Z",
          "shell.execute_reply.started": "2023-06-30T00:19:46.995963Z",
          "shell.execute_reply": "2023-06-30T00:19:47.014669Z"
        },
        "trusted": true,
        "id": "dslbkXmTrbuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTS**"
      ],
      "metadata": {
        "id": "v_7KR11YrbuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_generator = DataGenerator(list_IDs=X_train,\n",
        "                                    labels_dir='/kaggle/input/labelsssss/labels.csv',\n",
        "                                    dim_img=(192, 256),\n",
        "                                    batch_size=4,\n",
        "                                    sub_batch_size=300,\n",
        "                                    shuffle=True,\n",
        "                                    training=True)\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "with strategy.scope():\n",
        "    vgg19 = get_vgg19()\n",
        "    vgg19.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate=1e-7),\n",
        "                  metrics=[keras.metrics.AUC(name=f'AUC_{number_of_split+1}'),\n",
        "                           keras.metrics.Accuracy(name='accuracy'),\n",
        "                           keras.metrics.Precision(name='precision'),\n",
        "                           keras.metrics.Recall(name='recall')])\n",
        "\n",
        "\n",
        "# Train model on dataset\n",
        "vgg19.load_weights('/kaggle/working/models/cancer_auc3/') #start with best AUC in validations\n",
        "history = vgg19.fit(training_generator, epochs=10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-30T01:44:31.876246Z",
          "iopub.execute_input": "2023-06-30T01:44:31.876649Z",
          "iopub.status.idle": "2023-06-30T01:57:52.720778Z",
          "shell.execute_reply.started": "2023-06-30T01:44:31.876615Z",
          "shell.execute_reply": "2023-06-30T01:57:52.716189Z"
        },
        "trusted": true,
        "id": "S1u8Bt0prbuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = []\n",
        "for x in X_test:\n",
        "    x = np.load(x)\n",
        "    x = np.array(separate_slices(x))\n",
        "    pred = vgg19.predict(np.array(x), verbose=0)\n",
        "    res.append(np.argmax(pred))\n",
        "print(res)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-30T01:57:52.730291Z",
          "iopub.execute_input": "2023-06-30T01:57:52.731759Z",
          "iopub.status.idle": "2023-06-30T01:59:54.204725Z",
          "shell.execute_reply.started": "2023-06-30T01:57:52.731725Z",
          "shell.execute_reply": "2023-06-30T01:59:54.203711Z"
        },
        "trusted": true,
        "id": "smu7cCa9rbuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_confusion = np.array([[0, 0, 0, 0],\n",
        "                             [0, 0, 0, 0],\n",
        "                             [0, 0, 0, 0],\n",
        "                             [0, 0, 0, 0]])\n",
        "\n",
        "for y_pred, y_true in zip(res, y_test):\n",
        "    matrix_confusion[y_pred][y_true] += 1\n",
        "\n",
        "print(matrix_confusion)"
      ],
      "metadata": {
        "id": "fzzPgZNgAnUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}